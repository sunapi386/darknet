[net]
subdivisions=1
inputs=128
batch = 64
momentum=0.9
decay=0.001
max_batches = 2000
time_steps=576
learning_rate=0.1
policy=steps
steps=1000,1500
scales=.1,.1

[rnn]
batch_normalize=1
output = 128
hidden=128
activation=leaky

[rnn]
batch_normalize=1
output = 128
hidden=128
activation=leaky

[rnn]
batch_normalize=1
output = 128
hidden=128
activation=leaky

[connected]
output=128
activation=leaky

[softmax]

[cost]
type=sse

